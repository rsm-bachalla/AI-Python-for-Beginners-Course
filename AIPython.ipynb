{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/147.9 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 92.2/147.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 147.9/147.9 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\python312\\lib\\site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.8 MB 653.6 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.2/1.8 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.8 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 9.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-24.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.12.exe and pip3.exe are installed in 'c:\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.3.2 idna-3.10 requests-2.32.3 urllib3-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script normalizer.exe is installed in 'c:\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests # let's you download webpages into python\n",
    "from helper_functions import * \n",
    "from IPython.display import HTML, display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# The url from one of the Batch's newsletter\n",
    "url = 'https://www.deeplearning.ai/the-batch/the-world-needs-more-intelligence/'\n",
    "\n",
    "# Getting the content from the webpage's contents\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the response from the requests\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://www.deeplearning.ai/the-batch/the-world-needs-more-intelligence/ width=\"60%\" height=\"400\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(f'<iframe src={url} width=\"60%\" height=\"400\"></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ New course! Enroll in  Retrieval Optimization: From Tokenization to Vector Quantization\n",
      "Dear friends,\n",
      "Last year, a number of large businesses and individuals went to the media and governments and pushed the message that AI is scary, impossible to control, and might even lead to human extinction. Unfortunately they succeeded: Now many people think AI is scary. But when I speak with regulators, media, and private citizens, I like to bring the issue of whether AI is beneficial or harmful back to a very basic question: Are we better off with more, or less, intelligence in the world? \n",
      "Intelligence is the ability to apply skills and knowledge to make good decisions. Yes, intelligence can be used for nefarious purposes. But over many centuries, a major driver of civilization's progress has been people getting smarter and more educated. Until now, human intelligence has been the primary form of intelligence available. But with artificial intelligence, we have the opportunity to bring much more intelligence into the world. I discussed this opportunity in a recent interview (paywalled) with Financial Times reporter Ryan McMorrow.\n",
      "Historically, intelligence has been very expensive to acquire. It costs a lot to feed, raise, and train a broadly knowledgeable and experienced human being! That's why it’s so expensive to hire intelligence, such as a highly skilled doctor to examine and advise you on a medical condition, or a patient tutor who can understand your child and gently coach them where they need help. But with artificial intelligence, we have the potential to make intelligence cheap for everyone, so you no longer have to worry about a huge bill for seeing a doctor or educating your child. \n",
      "For society's biggest problems, such as climate change, intelligence — including artificial intelligence — also has a significant role to play. While having more intelligence in the world isn't the only thing (there are also nuances such as how to share the wealth it creates, how it will affect jobs, and how to keep it from being used for evil purposes), I believe we are much better off as a society with more intelligence, be it human or artificial intelligence. \n",
      "In my recent talk at TED AI (you can watch the 12-minute presentation here), I touched on why I'm excited about AI and why I think many of the anxieties about it are misplaced. If you speak with someone who’s worried about AI, please forward the talk to them to see if it helps to reassure them. Or ask if they fundamentally believe we want more intelligence in the world. I find that answering this question can be a useful North Star for how we approach AI.\n",
      "Keep learning!\n",
      "Andrew\n",
      "P.S. Check out our new short course on “Building Applications with Vector Databases,” taught by Pinecone’s Tim Tully! Vector databases (DBs) are commonly associated with retrieval augmented generation (RAG) but actually have many uses in AI applications. In this course, you’ll learn about (i) a basic semantic search app that uses a vector DB to find similar documents, (ii) a RAG application querying datasets it was not trained on, (iii) recommender systems that combine semantic search and RAG, (iv) hybrid search, which lets you work with dense and sparse vectors simultaneously, (v) anomaly detection applied to network logs, and (vi) an image-similarity application with a fun example that determines which parent a child resembles more. Come learn how you can use vector DBs to build many different types of applications! Enroll here\n",
      "Stay updated with weekly AI News and Insights delivered to your inbox\n"
     ]
    }
   ],
   "source": [
    "# Using beautifulsoup to extract the text\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# Find all the text in paragraph elements on the webpage\n",
    "all_text = soup.find_all('p')\n",
    "\n",
    "# Create an empty string to store the extracted text\n",
    "combined_text = \"\"\n",
    "\n",
    "# Iterate over 'all_text' and add to the combined_text string\n",
    "for text in all_text:\n",
    "    combined_text = combined_text + \"\\n\" + text.get_text()\n",
    "\n",
    "# Print the final combined text\n",
    "print(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: rsm-bachalla (Bindu Priyanka Achalla) · GitHub\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_page_title(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.title.get_text(strip=True) if soup.title else 'No title found'\n",
    "        return title\n",
    "    else:\n",
    "        return f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "\n",
    "url = 'https://github.com/rsm-bachalla'\n",
    "\n",
    "# Fetching and printing the title of the page\n",
    "page_title = get_page_title(url)\n",
    "print(\"Page Title:\", page_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: rsm-bachalla (Bindu Priyanka Achalla) · GitHub\n",
      "\n",
      "Headings:\n",
      "h1: ['Search code, repositories, users, issues, pull requests...', 'Provide feedback', 'Saved searches', 'Bindu Priyanka Achallarsm-bachalla', 'Block or report rsm-bachalla']\n",
      "h2: ['Navigation Menu', 'Use saved searches to filter your results more quickly', 'PinnedLoading', 'Footer']\n",
      "h3: ['Footer navigation']\n",
      "h4: []\n",
      "h5: []\n",
      "h6: []\n",
      "\n",
      "Paragraphs:\n",
      "We read every piece of feedback, and take your input very seriously.\n",
      "To see all available qualifiers, see ourdocumentation.\n",
      "\n",
      "Prevent this user from interacting with your repositories and sending you notifications.\n",
      "          Learn more aboutblocking users.\n",
      "You must be logged in to block users.\n",
      "Contact GitHub support about this user’s behavior.\n",
      "        Learn more aboutreporting abuse.\n",
      "Clothing Fit Prediction using Logistic Regression\n",
      "Jupyter Notebook\n",
      "\n",
      "\n",
      "\n",
      "Jupyter Notebook\n",
      "\n",
      "\n",
      "\n",
      "Jupyter Notebook\n",
      "\n",
      "Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "def scrape_webpage(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extracting the page title\n",
    "        title = soup.title.get_text(strip=True) if soup.title else 'No title found'\n",
    "\n",
    "        # Extracting all the headings (h1, h2, h3, etc.)\n",
    "        headings = {}\n",
    "        for i in range(1, 7):  # Looping through h1 to h6\n",
    "            headings[f'h{i}'] = [header.get_text(strip=True) for header in soup.find_all(f'h{i}')]\n",
    "\n",
    "        # Extracting all the paragraphs\n",
    "        paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "\n",
    "        # Returning the scraped details as a dictionary\n",
    "        return {\n",
    "            'title': title,\n",
    "            'headings': headings,\n",
    "            'paragraphs': paragraphs\n",
    "        }\n",
    "    else:\n",
    "        return f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "url = 'https://github.com/rsm-bachalla'  \n",
    "\n",
    "# Fetching and printing the details of the page\n",
    "page_details = scrape_webpage(url)\n",
    "\n",
    "# Displaying the scraped information\n",
    "print(\"Page Title:\", page_details['title'])\n",
    "print(\"\\nHeadings:\")\n",
    "for heading_level, heading_list in page_details['headings'].items():\n",
    "    print(f\"{heading_level}: {heading_list}\")\n",
    "print(\"\\nParagraphs:\")\n",
    "for paragraph in page_details['paragraphs']:\n",
    "    print(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: Magnolia Bakery’s Banana Pudding Recipe Is Totally Easy to Make at Home - Eater\n",
      "\n",
      "Headings:\n",
      "h1: ['Make the Banana Pudding That People Wait in Line for All Over the World']\n",
      "h2: ['Follow Eater online:', 'Site search', 'Eater main menu', 'Share this story', 'Magnolia Bakery Classic Banana Pudding Recipe', 'More FromEater', \"Sign up for theSign up for Eater's newsletter\", 'The Latest', 'In Defense of Fridgescaping', 'This South Indian Tasting Menu Serves a 1,000-Year-Old Dish', 'Every Single TikTok Grandma Is My Favorite', 'The Whole Foods ‘Chantilly Gate’ Saga Shows Complaining Works Sometimes', 'It’s Almost Prime Day... Again: Here’s What to Buy Now', 'The Cold Foam-ification of Fun Coffee Drinks', 'Share this story']\n",
      "h3: ['ShareAll sharing options for:Make the Banana Pudding That People Wait in Line for All Over the World', 'Ingredients:', 'Instructions:', 'Most Read']\n",
      "h4: ['Thanks for signing up!']\n",
      "h5: []\n",
      "h6: []\n",
      "\n",
      "Paragraphs:\n",
      "Filed under:\n",
      "New York-based Magnolia Bakery’s excellent pudding is easier to make than you think\n",
      "Extremely ripe bananas (along withsourdough) were among the must-have ingredients at the beginning of shelter-in-place orders, when it seemed likeeveryonewas bakingfor comfort. But while bread flour and sourdough starter becamemuch harderto find in the spring, we always have bananas —America’s most popular fruit— for our banana bread baking and, if you’re getting creative, building layers of banana pudding.\n",
      "According to culinary historian Robert Moss, bananas began to touch down in small numbers in the U.S. before the Civil War, arriving by boat from the West Indies. Then, thanks todecades of aggressive ad campaignstouting the banana’s nutritional benefits, demand for the delicate, tropical fruit skyrocketed and bananas became an unmovable part of the American diet. By the end of the 1800s, over four million bunches were arriving annually to ports, with New Orleans enjoying the greatest share, and more and more cooks were getting creative with how to use them.\n",
      "Some say banana pudding’s reputation as a “Southern food” isdue to its arrival in Southern ports, like NOLA’s. Additionally, Moss makes the case that the ease of making banana pudding — in large batches, without having to turn the oven on — made it ideal for serving at large Southern social gatherings and events.\n",
      "But recipes for banana pudding were in the North as early as the 1880s and variations abound, from using lady fingers in place of sponge cake (the original base) to topping it with meringue or infusing it with citrus. In 1921, Laura Kerley published a banana pudding recipe that swapped in vanilla wafers as the base; Nabisco jumped aboard and was publishing a recipe for banana puddingon the side of its wafer boxesby the 1940s. By the ’50s and ’60s, the banana pudding we know today was the norm: bananas, vanilla wafers, custard, and whipped cream.\n",
      "Today, the most famous maker of banana pudding is probably New York-basedMagnolia Bakery. Though it shot to fame thanks to its cupcakes and acouple of pop culture references, it’s the excellent (and arguably better) banana pudding that keeps lines snaking at nearly 20 locations worldwide. You can find the recipe for Magnolia’s version below, prepared by chief baking officer Bobbie Lloyd. Magnolia alsooffers kits to help recreatethe signature recipe at home.\n",
      "1 14-ounce can sweetened condensed milk1½ cups (360 grams/12.7 oz) ice cold water1 3.4-ounce package instant vanilla pudding mix, preferably Jell-O Brand3 cups (720 grams/25.5 oz) heavy cream1 11-ounce box vanilla wafers, preferably Nilla brand4 to 5 ripe bananas, sliced\n",
      "Step 1:In a medium sized bowl whisk together the sweetened condensed milk and the cold water. Place the pudding mix in another medium sized bowl and slowly whisk in the liquid, whisking until there are no lumps and the mixture is smooth, about 1 minute. Cover and refrigerate until firm at least an hour or overnight.\n",
      "Step 2:In a stand mixer with the whisk attachment or using a hand mixer, whip the heavy cream on medium speed for about 1 minute until the cream starts to thicken. Increase the speed to medium-high and whip the heavy cream until stiff peaks form. Be careful not to over-whip.\n",
      "Step 3:Carefully add the pudding mixture to the whipped cream and mix on low speed until blended and no streaks of pudding remain.\n",
      "Step 4:Using either a trifle bowl or a wide glass bowl with a 4- to 5-quart capacity, spread one-quarter of the pudding over the bottom and layer with one-third of the cookies [save 4 to 5 cookies for the garnish on top] and about 1 to 11⁄2of the sliced bananas — enough to cover the layer. Repeat the layering twice more. End with a final layer of pudding. Garnish the top with additional cookies or cookie crumbs.\n",
      "Step 5:Cover tightly with plastic wrap and refrigerate for 4 to 6 hours. Cookies should be tender when poked with a knife. It’s best served within 12 hours of assembling. Enjoy!\n",
      "Sign up to hear about more Instagram cook-alongs\n",
      "The freshest news from the food world every day\n",
      "Check your inbox for a welcome email.\n",
      "Oops. Something went wrong. Please enter a valid email and try again.\n"
     ]
    }
   ],
   "source": [
    "def scrape_webpage(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extracting the page title\n",
    "        title = soup.title.get_text(strip=True) if soup.title else 'No title found'\n",
    "\n",
    "        # Extracting all the headings (h1, h2, h3, etc.)\n",
    "        headings = {}\n",
    "        for i in range(1, 7):  # Looping through h1 to h6\n",
    "            headings[f'h{i}'] = [header.get_text(strip=True) for header in soup.find_all(f'h{i}')]\n",
    "\n",
    "        # Extracting all the paragraphs\n",
    "        paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "\n",
    "        # Returning the scraped details as a dictionary\n",
    "        return {\n",
    "            'title': title,\n",
    "            'headings': headings,\n",
    "            'paragraphs': paragraphs\n",
    "        }\n",
    "    else:\n",
    "        return f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n",
    "\n",
    "url = 'https://www.eater.com/21449002/world-famous-magnolia-bakery-banana-pudding-recipe'  \n",
    "\n",
    "# Fetching and printing the details of the page\n",
    "page_details = scrape_webpage(url)\n",
    "\n",
    "# Displaying the scraped information\n",
    "print(\"Page Title:\", page_details['title'])\n",
    "print(\"\\nHeadings:\")\n",
    "for heading_level, heading_list in page_details['headings'].items():\n",
    "    print(f\"{heading_level}: {heading_list}\")\n",
    "print(\"\\nParagraphs:\")\n",
    "for paragraph in page_details['paragraphs']:\n",
    "    print(paragraph)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
